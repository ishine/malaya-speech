{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d338288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('canopylabs/orpheus-3b-0.1-ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a386404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd63e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('mesolitica/TTS-Combined')['train']\n",
    "rows = dataset.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c6e682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filename': 'husein-chatbot/husein-chatbot-politics-normalized-v2-5674.mp3',\n",
       " 'prompt': 'Husein',\n",
       " 'transcription': 'Berikut ialah beberapa cara langkah - langkah ini boleh dilaksanakan dengan berkesan ,'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ff3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def new_path(f):\n",
    "    f = f.replace('.mp3', '.snac')\n",
    "    splitted = f.split('/')\n",
    "    base_folder = splitted[0] + '_snac'\n",
    "    splitted = '/'.join([base_folder] + splitted[1:])\n",
    "    return splitted\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tokenized-3k’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir tokenized-3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5305c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "block_size = 3072\n",
    "\n",
    "def loop(rows, block_size = block_size):\n",
    "    rows, index = rows\n",
    "    out_root = f'tokenized-3k/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            speaker = row['prompt']\n",
    "            text = row['transcription']\n",
    "            with open(new_path(row['audio_filename'])) as fopen:\n",
    "                myts = json.load(fopen)\n",
    "            prompt = f'<custom_token_3><|begin_of_text|>{speaker}: {text}<|eot_id|><custom_token_4><custom_token_5><custom_token_1>'\n",
    "            outputs = tokenizer(prompt, add_special_tokens = False, return_attention_mask = False)\n",
    "            outputs = outputs['input_ids'] + myts + [128258]\n",
    "            temp.append(outputs)\n",
    "            position_ids.append(range(len(outputs)))\n",
    "            count += len(outputs)\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "                \n",
    "        block, _ = slice_and_balance(last_block, block_size - count)\n",
    "        block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "\n",
    "        block.extend(temp)\n",
    "        block_position.extend(position_ids)\n",
    "\n",
    "        o = collator(block, block_position)\n",
    "        if len(o['input_ids']) == block_size:\n",
    "            out.write(o)\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b5519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 6590.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([148634, 149446, 154746, ..., 152348, 154697, 128258], dtype=uint32),\n",
       " 'position_ids': array([491, 492, 493, ..., 471, 472, 473], dtype=uint32),\n",
       " 'attention_mask': array([228, 613, 450, 109, 811, 387, 474], dtype=uint32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop((rows[:1000], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd711cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': array([ 483,  876, 1177,  536], dtype=uint32),\n",
       " 'input_ids': array([128259, 128000,     39, ..., 134495, 139889, 141470], dtype=uint32),\n",
       " 'position_ids': array([  0,   1,   2, ..., 533, 534, 535], dtype=uint32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dataset = LocalDataset('tokenized-3k/tokenized-0')\n",
    "local_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e36301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 43%|██████████████████████████████████████▋                                                   | 21503/50000 [00:04<00:08, 3412.48it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:07<00:00, 6295.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5510.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:11<00:00, 4524.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:10<00:00, 4874.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:10<00:00, 4633.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5094.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:13<00:00, 3728.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:10<00:00, 4781.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:11<00:00, 4282.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:10<00:00, 4869.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5247.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:08<00:00, 5568.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 46010/46010 [00:08<00:00, 5435.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "chunks = chunks(rows, 50000)\n",
    "pool = Pool(10)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ecab7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenized-3k/tokenized-0',\n",
       " 'tokenized-3k/tokenized-1',\n",
       " 'tokenized-3k/tokenized-2',\n",
       " 'tokenized-3k/tokenized-3',\n",
       " 'tokenized-3k/tokenized-4',\n",
       " 'tokenized-3k/tokenized-5',\n",
       " 'tokenized-3k/tokenized-6',\n",
       " 'tokenized-3k/tokenized-7',\n",
       " 'tokenized-3k/tokenized-8',\n",
       " 'tokenized-3k/tokenized-9',\n",
       " 'tokenized-3k/tokenized-10',\n",
       " 'tokenized-3k/tokenized-11',\n",
       " 'tokenized-3k/tokenized-12']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted(glob('tokenized-3k/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24edc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf packing-3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a201470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10304/10304 [00:00<00:00, 19355.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10277/10277 [00:00<00:00, 16557.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10282/10282 [00:00<00:00, 16591.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10274/10274 [00:00<00:00, 16502.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10289/10289 [00:00<00:00, 19333.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10271/10271 [00:00<00:00, 16599.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10278/10278 [00:00<00:00, 16075.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10290/10290 [00:00<00:00, 16246.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10330/10330 [00:00<00:00, 19191.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10275/10275 [00:00<00:00, 15824.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10266/10266 [00:00<00:00, 16348.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 10338/10338 [00:00<00:00, 16361.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 9475/9475 [00:00<00:00, 15637.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='packing-3k', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36318062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408419328"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('packing-3k')\n",
    "(len(dataset) * block_size) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66066793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"packing-3k\",\n",
    "    repo_id=\"huseinzol05/orpheus-3k-multipacking\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750236c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
