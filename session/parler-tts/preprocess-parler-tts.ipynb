{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb395bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, HfArgumentParser\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "from transformers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5179bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parler_tts import (\n",
    "    ParlerTTSConfig,\n",
    "    ParlerTTSForConditionalGeneration,\n",
    "    build_delay_pattern_mask,\n",
    ")\n",
    "\n",
    "from training.utils import (\n",
    "    get_last_checkpoint,\n",
    "    rotate_checkpoints,\n",
    "    log_pred,\n",
    "    log_metric,\n",
    "    load_all_codec_checkpoints,\n",
    "    save_codec_checkpoint,\n",
    "    get_last_codec_checkpoint_step,\n",
    ")\n",
    "from accelerate import Accelerator, skip_first_batches\n",
    "from accelerate.utils import set_seed, AutocastKwargs, InitProcessGroupKwargs, TorchDynamoPlugin\n",
    "from accelerate.utils.memory import release_memory\n",
    "from training.arguments import ModelArguments, DataTrainingArguments, ParlerTTSTrainingArguments\n",
    "from training.data import load_multiple_datasets, DataCollatorParlerTTSWithPadding, DataCollatorEncodecWithPadding\n",
    "from training.eval import clap_similarity, wer, si_sdr\n",
    "from datasets import Dataset, IterableDataset, concatenate_datasets, interleave_datasets, load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae903a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision = \"bf16\"\n",
    "torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a581a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415b905407264ed389b3bafd4d7ba491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    'parler-tts/parler-tts-mini-v1'\n",
    ")\n",
    "sampling_rate = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7b1bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6e2d43f6fc4879acbafb7feebb04c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4207aaa9806340f6ab2e4658b4181264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd55b48acfe49b9b2c002987eba74c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b912823962c348c5916a0a283a4a449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'google/flan-t5-large',\n",
    "    padding_side='left',\n",
    ")\n",
    "\n",
    "# load description tokenizer\n",
    "description_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'google/flan-t5-large',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "134f2822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c0cd9fae246028e12f0b6dcf94b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = ParlerTTSConfig.from_pretrained(\n",
    "    'parler-tts/parler-tts-mini-v1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0a282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4671d5db1b1640e3ae17be610abb90c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
    "    'parler-tts/parler-tts-mini-v1',\n",
    "    attn_implementation='sdpa',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f341ad2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 21 20:01:09 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   34C    P8             22W /  350W |   12628MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:08:00.0 Off |                  Off |\n",
      "|  0%   44C    P8             23W /  300W |       4MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2415547      C   python3                                     12610MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e81677e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d50a867b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision=mixed_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93432ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccd2f98f1b549b9a86b5c3b17077d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/360248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with accelerator.local_main_process_first():\n",
    "    raw_datasets = load_dataset(\n",
    "        'mesolitica/tts-combine-annotated', split = 'train'\n",
    "    )\n",
    "    \n",
    "raw_datasets = raw_datasets.filter(lambda x: x['prompt'] is not None and x['transcription'] is not None)\n",
    "raw_datasets = raw_datasets.filter(lambda x: len(x['prompt']) < 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "767c5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def check_len(f):\n",
    "    y, sr = sf.read(f)\n",
    "    return len(y) / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e348595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f848247a4d445009f04cf743fb0e330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=10):   0%|          | 0/358348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = raw_datasets.filter(lambda x: 0 < check_len(x['audio_filename']) < 30, num_proc = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19e414cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2b247073af451884f7eddda13a8312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess datasets (num_proc=5):   0%|          | 0/358348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def pass_through_processors(description, prompt):\n",
    "    batch = {}\n",
    "\n",
    "    batch[\"input_ids\"] = description_tokenizer(description.strip())[\"input_ids\"]\n",
    "    batch[\"prompt_input_ids\"] = prompt_tokenizer(prompt.strip())[\"input_ids\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "with accelerator.local_main_process_first():\n",
    "    vectorized_datasets = raw_datasets.map(\n",
    "        pass_through_processors,\n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        input_columns=['prompt', 'transcription'],\n",
    "        num_proc=5,\n",
    "        desc=\"preprocess datasets\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "866e8956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'prompt_input_ids'],\n",
       "    num_rows: 358348\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "365be38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 358348/358348 [00:52<00:00, 6885.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "speakers = defaultdict(list)\n",
    "for i in tqdm(range(len(raw_datasets))):\n",
    "    speakers[raw_datasets[i]['speaker']].append(raw_datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9b7b840",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052.65\n",
      "1052.98\n",
      "309.39\n",
      "309.01\n",
      "309.46\n",
      "308.55\n",
      "117.02\n",
      "124.42\n"
     ]
    }
   ],
   "source": [
    "for speaker in speakers.keys():\n",
    "    print(len(speakers[speaker]) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6394e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 105265/105265 [00:00<00:00, 6430243.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 105298/105298 [00:00<00:00, 6622360.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 30939/30939 [00:00<00:00, 6655429.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 30901/30901 [00:00<00:00, 6826154.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 30946/30946 [00:00<00:00, 6699888.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 30855/30855 [00:00<00:00, 6713802.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 11702/11702 [00:00<00:00, 6807454.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 12442/12442 [00:00<00:00, 6328587.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train, test = [], []\n",
    "for speaker in speakers.keys():\n",
    "    for row in tqdm(speakers[speaker]):\n",
    "        if random.random() > 0.01:\n",
    "            train.append(row)\n",
    "        else:\n",
    "            test.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e582694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354705, 3643)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1da6738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcription': 'lidah yang diam dan mendengar sahaja itu Ketawa pula Lalu dia menerangkan',\n",
       " 'speaker': 'Elina',\n",
       " 'speaker_id': 7,\n",
       " 'gender': 'female',\n",
       " 'utterance_pitch_mean': 149.9508819580078,\n",
       " 'utterance_pitch_std': 29.23186683654785,\n",
       " 'snr': 70.31266021728516,\n",
       " 'c50': 31.286865234375,\n",
       " 'speech_duration': 6.142500000000002,\n",
       " 'stoi': 0.9906082153320312,\n",
       " 'si-sdr': 22.291345596313477,\n",
       " 'pesq': 3.2068662643432617,\n",
       " 'pitch': 'very low pitch',\n",
       " 'speaking_rate': 'very slowly',\n",
       " 'noise': 'very clear',\n",
       " 'reverberation': 'quite roomy sounding',\n",
       " 'speech_monotony': 'very monotone',\n",
       " 'prompt': 'Elina, a female speaker delivers an expressive and animated speech in a room with slight background noise. Her voice is quite roomy-sounding, with some clarity present. She speaks very slowly with a very low-pitch, but a very monotone tone.',\n",
       " 'audio_filename': 'combine-audio/360226.mp3'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d4bd0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_list(train),\n",
    "    'test': Dataset.from_list(test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7df68931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caba71273c0141e0b13b628bea8404e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e6242d0ed4394961ef86e7f2a854b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/355 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2bc00389a14dcda8089a60c2f756d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c5efd4f4ee4067b29fa65e0a85ddc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f86b31c46f42b4b4f1e3dad56bc571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/huseinzol05/processed-tts-combine-annotated/commit/2cf4fac860b3d1382ae52ed3ea3020809746dfb9', commit_message='Upload dataset', commit_description='', oid='2cf4fac860b3d1382ae52ed3ea3020809746dfb9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.push_to_hub('huseinzol05/processed-tts-combine-annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9bf15538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.data import load_multiple_datasets, DataCollatorParlerTTSWithPadding, DataCollatorEncodecWithPadding\n",
    "\n",
    "feature_extractor_input_name = feature_extractor.model_input_names[0]\n",
    "max_target_length = 30 * sampling_rate\n",
    "padding = \"longest\"\n",
    "max_length = model.generation_config.max_length\n",
    "num_codebooks = model.decoder.config.num_codebooks\n",
    "audio_encoder_bos_token_id = model.generation_config.decoder_start_token_id\n",
    "bandwidth = 6\n",
    "encoder_data_collator = DataCollatorEncodecWithPadding(\n",
    "    feature_extractor,\n",
    "    audio_column_name='audio',\n",
    "    feature_extractor_input_name=feature_extractor_input_name,\n",
    "    max_length=max_target_length,\n",
    "    padding=padding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "72e96a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_audio_decoder(batch):\n",
    "    len_audio = batch.pop(\"len_audio\")\n",
    "    audio_decoder.to(batch[\"input_values\"].device).eval()\n",
    "    with torch.no_grad():\n",
    "        labels = audio_decoder.encode(**batch, bandwidth=bandwidth)[\"audio_codes\"]\n",
    "    output = {}\n",
    "    output[\"len_audio\"] = len_audio\n",
    "    # (1, bsz, codebooks, seq_len) -> (bsz, seq_len, codebooks)\n",
    "    output[\"labels\"] = labels.squeeze(0).transpose(1, 2)\n",
    "\n",
    "    # if `pad_to_max_length`, the maximum corresponding audio length of the current batch is max_duration*sampling_rate\n",
    "    max_length = len_audio.max() if padding != \"max_length\" else max_target_length\n",
    "    output[\"ratio\"] = torch.ones_like(len_audio) * labels.shape[-1] / max_length\n",
    "    return output\n",
    "\n",
    "# (1, codebooks, seq_len) where seq_len=1\n",
    "bos_labels = torch.ones((1, num_codebooks, 1)) * audio_encoder_bos_token_id\n",
    "\n",
    "def postprocess_dataset(labels):\n",
    "    # (1, codebooks, seq_len)\n",
    "    labels = torch.tensor(labels).unsqueeze(0)\n",
    "    # add bos\n",
    "    labels = torch.cat([bos_labels, labels], dim=-1)\n",
    "\n",
    "    labels, delay_pattern_mask = build_delay_pattern_mask(\n",
    "        labels,\n",
    "        bos_token_id=audio_encoder_bos_token_id,\n",
    "        pad_token_id=audio_encoder_eos_token_id,\n",
    "        max_length=labels.shape[-1] + num_codebooks,\n",
    "        num_codebooks=num_codebooks,\n",
    "    )\n",
    "\n",
    "    # the first ids of the delay pattern mask are precisely labels, we use the rest of the labels mask\n",
    "    # to take care of EOS\n",
    "    # we want labels to look like this:\n",
    "    #  - [B, a, b, E, E, E, E]\n",
    "    #  - [B, B, c, d, E, E, E]\n",
    "    #  - [B, B, B, e, f, E, E]\n",
    "    #  - [B, B, B, B, g, h, E]\n",
    "    labels = torch.where(delay_pattern_mask == -1, audio_encoder_eos_token_id, delay_pattern_mask)\n",
    "\n",
    "    # the first timestamp is associated to a row full of BOS, let's get rid of it\n",
    "    # we also remove the last timestampts (full of PAD)\n",
    "    output = {\"labels\": labels[:, 1:]}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "efe93a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset_dict['train'].rename_column('audio_filename', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd3ff606",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.cast_column(\"audio\", Audio(sampling_rate = feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a65a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32), 'input_values': tensor([[[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]],\n",
       "\n",
       "        [[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]],\n",
       "\n",
       "        [[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]],\n",
       "\n",
       "        [[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]],\n",
       "\n",
       "        [[-0.0008, -0.0012, -0.0015,  ...,  0.0005,  0.0003,  0.0001]]]), 'len_audio': tensor([[314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346],\n",
       "        [314346]])}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [d[i] for i in range(10)]\n",
    "batch = encoder_data_collator(batch)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5b7a6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_decoder = model.audio_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "936e50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_labels = apply_audio_decoder(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6c5d6e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len_audio': tensor([[314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346],\n",
       "         [314346]]),\n",
       " 'labels': tensor([[[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]],\n",
       " \n",
       "         [[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]],\n",
       " \n",
       "         [[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]],\n",
       " \n",
       "         [[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]],\n",
       " \n",
       "         [[698, 710, 114,  ..., 496, 387, 348],\n",
       "          [698, 249, 540,  ..., 947, 803, 902],\n",
       "          [698, 578, 888,  ..., 428, 570, 683],\n",
       "          ...,\n",
       "          [568, 778, 771,  ..., 338, 378, 731],\n",
       "          [698, 151, 229,  ..., 145, 954, 726],\n",
       "          [698, 847, 408,  ..., 138,  83, 640]]]),\n",
       " 'ratio': tensor([[0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020],\n",
       "         [0.0020]])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
