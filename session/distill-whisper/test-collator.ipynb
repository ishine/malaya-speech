{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19fad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AddedToken,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizerFast,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "import json\n",
    "from datasets import Audio\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8587e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-large-v3')\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained('openai/whisper-large-v3')\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v3')\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "config = WhisperConfig.from_pretrained('openai/whisper-large-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95e5b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = []\n",
    "        with open(file) as fopen:\n",
    "            for l in fopen:\n",
    "                self.data.append(json.loads(l))\n",
    "\n",
    "        self.audio = Audio(sampling_rate=16000)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio = self.audio.decode_example(\n",
    "            self.audio.encode_example(\n",
    "                self.data[item]['audio_filename']))['array']\n",
    "        inputs = feature_extractor(audio, sampling_rate=sampling_rate)\n",
    "        if self.data[item]['score_ms'] >= self.data[item]['score_en']:\n",
    "            input_str = self.data[item]['predict_ms']\n",
    "        else:\n",
    "            input_str = self.data[item]['predict_en']\n",
    "\n",
    "        token_ids = tokenizer(input_str, add_special_tokens=False).input_ids\n",
    "\n",
    "        return {\n",
    "            'input_features': inputs.input_features[0],\n",
    "            'input_length': [len(audio)],\n",
    "            'labels': token_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f9bbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Train('sample-set.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46e69bb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': array([[ 0.2577381 ,  0.12420219, -0.0346607 , ..., -0.02739227,\n",
       "         -0.04189217,  0.03024662],\n",
       "        [ 0.3553025 ,  0.22176659,  0.0629037 , ...,  0.07017213,\n",
       "          0.05567229,  0.12781101],\n",
       "        [ 0.38467056,  0.5888243 ,  0.55428416, ...,  0.35781485,\n",
       "          0.14236629,  0.29945827],\n",
       "        ...,\n",
       "        [ 0.01472914, -0.4861127 , -0.53696144, ..., -0.39464724,\n",
       "         -0.51083815, -0.345137  ],\n",
       "        [-0.01119864, -0.52233994, -0.53696144, ..., -0.53696144,\n",
       "         -0.53696144, -0.38162374],\n",
       "        [-0.01916075, -0.5298321 , -0.53696144, ..., -0.53696144,\n",
       "         -0.53696144, -0.38669133]], dtype=float32),\n",
       " 'input_length': [480000],\n",
       " 'labels': [50258,\n",
       "  50282,\n",
       "  50360,\n",
       "  21851,\n",
       "  27294,\n",
       "  803,\n",
       "  2760,\n",
       "  1063,\n",
       "  282,\n",
       "  25835,\n",
       "  20451,\n",
       "  17922,\n",
       "  7834,\n",
       "  4072,\n",
       "  647,\n",
       "  1538,\n",
       "  803,\n",
       "  2760,\n",
       "  1063,\n",
       "  282,\n",
       "  803,\n",
       "  86,\n",
       "  17017,\n",
       "  44988,\n",
       "  963,\n",
       "  64,\n",
       "  9160,\n",
       "  18943,\n",
       "  16281,\n",
       "  1706,\n",
       "  66,\n",
       "  3504,\n",
       "  23063,\n",
       "  28042,\n",
       "  409,\n",
       "  9160,\n",
       "  16281,\n",
       "  1706,\n",
       "  66,\n",
       "  3504,\n",
       "  803,\n",
       "  23063,\n",
       "  6468,\n",
       "  24585,\n",
       "  22823,\n",
       "  40069,\n",
       "  7961,\n",
       "  17240,\n",
       "  4468,\n",
       "  1694,\n",
       "  5948,\n",
       "  70,\n",
       "  394,\n",
       "  1063,\n",
       "  26069,\n",
       "  21310,\n",
       "  284,\n",
       "  2253,\n",
       "  10770,\n",
       "  21851,\n",
       "  19377,\n",
       "  2095,\n",
       "  44988,\n",
       "  350,\n",
       "  1805,\n",
       "  656,\n",
       "  1026,\n",
       "  71,\n",
       "  394,\n",
       "  289,\n",
       "  18943,\n",
       "  15330,\n",
       "  19619,\n",
       "  2794,\n",
       "  274,\n",
       "  82,\n",
       "  81,\n",
       "  9160,\n",
       "  369,\n",
       "  73,\n",
       "  1459,\n",
       "  71,\n",
       "  7408,\n",
       "  19399,\n",
       "  29047,\n",
       "  15951,\n",
       "  1366,\n",
       "  8021,\n",
       "  9160,\n",
       "  12711,\n",
       "  31161,\n",
       "  22602,\n",
       "  5225,\n",
       "  803,\n",
       "  86,\n",
       "  17017,\n",
       "  14979,\n",
       "  9900,\n",
       "  19399,\n",
       "  32633,\n",
       "  992,\n",
       "  338,\n",
       "  545,\n",
       "  4795,\n",
       "  41650,\n",
       "  369,\n",
       "  7124,\n",
       "  4883,\n",
       "  9160,\n",
       "  1706,\n",
       "  443,\n",
       "  3077,\n",
       "  369,\n",
       "  37909,\n",
       "  3435,\n",
       "  73,\n",
       "  901,\n",
       "  5581,\n",
       "  35161,\n",
       "  1706,\n",
       "  325,\n",
       "  1010,\n",
       "  72,\n",
       "  680,\n",
       "  71,\n",
       "  836,\n",
       "  1063,\n",
       "  282,\n",
       "  23319,\n",
       "  35891,\n",
       "  3765,\n",
       "  46291,\n",
       "  1706,\n",
       "  260,\n",
       "  4775,\n",
       "  5285,\n",
       "  4,\n",
       "  15597,\n",
       "  9032,\n",
       "  9160,\n",
       "  25835,\n",
       "  27942,\n",
       "  320,\n",
       "  289,\n",
       "  680,\n",
       "  650,\n",
       "  2394,\n",
       "  282,\n",
       "  13877,\n",
       "  15330,\n",
       "  7414,\n",
       "  14910,\n",
       "  261,\n",
       "  656,\n",
       "  7408,\n",
       "  19377,\n",
       "  2095,\n",
       "  45016,\n",
       "  17742,\n",
       "  20721,\n",
       "  46975,\n",
       "  301,\n",
       "  545,\n",
       "  5225,\n",
       "  9160,\n",
       "  3277,\n",
       "  382,\n",
       "  302,\n",
       "  23319,\n",
       "  35891,\n",
       "  361,\n",
       "  5439,\n",
       "  21851,\n",
       "  18943,\n",
       "  5851,\n",
       "  13708,\n",
       "  13877,\n",
       "  1706,\n",
       "  66,\n",
       "  3504,\n",
       "  50257]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "743a3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`Wav2Vec2Processor`])\n",
    "            The processor used for proccessing the data.\n",
    "        decoder_start_token_id (:obj: `int`)\n",
    "            The start-of-sequence token id of the decoder.\n",
    "        decoder_prev_token_id (:obj: `int`)\n",
    "            The start-of-prompt token id of the decoder\n",
    "        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).\n",
    "            See above for details.\n",
    "        max_target_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    decoder_prev_token_id: int\n",
    "    input_padding:str = \"max_length\"\n",
    "    target_padding:str = \"max_length\"\n",
    "    max_target_length:int = None\n",
    "\n",
    "    def __call__(\n",
    "            self, features):\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "\n",
    "        # dataloader returns a list of features which we convert to a dict\n",
    "        input_features = {model_input_name: [feature[model_input_name] for feature in features]}\n",
    "        label_features = {\"input_ids\": [feature[\"labels\"] for feature in features]}\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.input_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # shift labels to the right to get decoder input ids\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        decoder_input_ids = labels[:, :-1]\n",
    "        labels = labels[:, 1:]\n",
    "        labels_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = labels.masked_fill(labels_mask.ne(1), -100)\n",
    "\n",
    "        # replace initial prompt tokens with -100 to ignore correctly when computing the loss\n",
    "        bos_index = torch.argmax((labels == self.decoder_start_token_id).long(), dim=1)\n",
    "        prompt_mask = torch.arange(labels.shape[1]) < bos_index[:, None]\n",
    "        labels = torch.where(prompt_mask, -100, labels)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "527d171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_length = 384\n",
    "decoder_start_token_id = config.decoder_start_token_id\n",
    "decoder_prev_token_id = tokenizer.all_special_ids[-3]\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    decoder_prev_token_id=decoder_prev_token_id,\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=max_label_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a1411eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[ 0.2577,  0.1242, -0.0347,  ..., -0.0274, -0.0419,  0.0302],\n",
       "         [ 0.3553,  0.2218,  0.0629,  ...,  0.0702,  0.0557,  0.1278],\n",
       "         [ 0.3847,  0.5888,  0.5543,  ...,  0.3578,  0.1424,  0.2995],\n",
       "         ...,\n",
       "         [ 0.0147, -0.4861, -0.5370,  ..., -0.3946, -0.5108, -0.3451],\n",
       "         [-0.0112, -0.5223, -0.5370,  ..., -0.5370, -0.5370, -0.3816],\n",
       "         [-0.0192, -0.5298, -0.5370,  ..., -0.5370, -0.5370, -0.3867]],\n",
       "\n",
       "        [[ 0.3003, -0.0457,  0.0751,  ...,  0.0324,  0.0659,  0.1541],\n",
       "         [ 0.3979,  0.0518,  0.1726,  ...,  0.1299,  0.1635,  0.2517],\n",
       "         [ 0.3618, -0.0771,  0.2485,  ...,  0.5078,  0.4999,  0.5185],\n",
       "         ...,\n",
       "         [ 0.1584, -0.3193, -0.5183,  ..., -0.5183, -0.5183, -0.5183],\n",
       "         [ 0.1155, -0.3962, -0.5183,  ..., -0.5183, -0.5183, -0.5183],\n",
       "         [ 0.1029, -0.4082, -0.5183,  ..., -0.5183, -0.5183, -0.5183]],\n",
       "\n",
       "        [[ 0.7460, -0.0912,  0.0428,  ..., -0.0328,  0.1653, -0.0675],\n",
       "         [ 0.8435,  0.0064,  0.1404,  ...,  0.0648,  0.2629,  0.0300],\n",
       "         [ 0.8610,  0.5551,  0.5368,  ...,  0.4601,  0.5991,  0.5483],\n",
       "         ...,\n",
       "         [-0.2778, -0.5221, -0.5221,  ..., -0.5221, -0.5221, -0.5221],\n",
       "         [-0.3259, -0.5221, -0.5221,  ..., -0.5221, -0.5221, -0.5221],\n",
       "         [-0.3389, -0.5221, -0.5221,  ..., -0.5221, -0.5221, -0.5221]]]), 'labels': tensor([[50282, 50360, 21851,  ...,  -100,  -100,  -100],\n",
       "        [50282, 50360, 21851,  ...,  -100,  -100,  -100],\n",
       "        [50282, 50360,  3277,  ...,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[50258, 50282, 50360,  ..., 50257, 50257, 50257],\n",
       "        [50258, 50282, 50360,  ..., 50257, 50257, 50257],\n",
       "        [50258, 50282, 50360,  ..., 50257, 50257, 50257]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [train_dataset[i] for i in range(3)]\n",
    "batch = data_collator(batch)\n",
    "batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
